service: tabelog-scraping
frameworkVersion: "2"

provider:
  name: aws
  runtime: python3.8
  region: ap-northeast-1
  lambdaHashingVersion: 20201221
  environment:
    TZ: Asia/Tokyo
    GET_URL_REQUEST_SQS_URL: { Ref: GetUrlRequestQueue }
    SCRAPE_REQUEST_SQS_URL: { Ref: ScrapeRequestQueue }
    S3_INPUT_BUCKET: tabelog-scraping-input
    S3_OUTPUT_BUCKET: tabelog-scraping-output
    DB_RST_URL_TABLE: TabelogRstUrl
    DB_RST_DATA_TABLE: TabelogRstData
  iam:
    role:
      statements:
        - Effect: Allow
          Action:
            - s3:*
          Resource:
            - "arn:aws:s3:::${self:provider.environment.S3_INPUT_BUCKET}/*"
            - "arn:aws:s3:::${self:provider.environment.S3_OUTPUT_BUCKET}/*"

        - Effect: "Allow"
          Action:
            - dynamodb:*
          Resource:
            - "arn:aws:dynamodb:${opt:region, self:provider.region}:*:table/TabelogRstUrl"
            - "arn:aws:dynamodb:${opt:region, self:provider.region}:*:table/TabelogRstData"

        - Effect: "Allow"
          Action:
            - "sqs:*"
          Resource:
            - "arn:aws:sqs:${opt:region, self:provider.region}:*:get_url_request"
            - "arn:aws:sqs:${opt:region, self:provider.region}:*:scrape_request"

plugins:
  - serverless-python-requirements

functions:
  publish_get_url_request_by_s3:
    handler: src/publish_get_url_request_by_s3.handler
    timeout: 120
    events:
      - s3:
          bucket: ${self:provider.environment.S3_INPUT_BUCKET}
          event: s3:ObjectCreated:*
          existing: true
  publish_get_url_request_by_http:
    handler: src/publish_get_url_request_by_http.handler
    timeout: 120
    events:
      - http:
          path: url
          method: get
          cors: true
  get_url:
    handler: src/get_url.handler
    timeout: 60
    events:
      - sqs:
          arn: { Fn::GetAtt: [GetUrlRequestQueue, Arn] }
  scrape:
    handler: src/scrape.handler
    timeout: 60
    events:
      - sqs:
          arn: { Fn::GetAtt: [ScrapeRequestQueue, Arn] }
  publish_scrape_request:
    handler: src/publish_scrape_request.handler
    timeout: 120
    events:
      - http:
          path: scrape
          method: get
          cors: true
  dump_to_csv:
    handler: src/dump_to_csv.handler
    timeout: 120
    events:
      - http:
          path: csv
          method: get
          cors: true

resources:
  Resources:
    # S3
    InputBucket:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: ${self:provider.environment.S3_INPUT_BUCKET}
    OutputBucket:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: ${self:provider.environment.S3_OUTPUT_BUCKET}

    # SQS
    GetUrlRequestQueue:
      Type: "AWS::SQS::Queue"
      Properties:
        QueueName: "get_url_request"
        MessageRetentionPeriod: 300
        ReceiveMessageWaitTimeSeconds: 20
        VisibilityTimeout: 60
    ScrapeRequestQueue:
      Type: "AWS::SQS::Queue"
      Properties:
        QueueName: "scrape_request"
        MessageRetentionPeriod: 300
        ReceiveMessageWaitTimeSeconds: 20
        VisibilityTimeout: 60

    # DynamoDB
    TabelogRstUrlTable:
      Type: AWS::DynamoDB::Table
      Properties:
        TableName: ${self:provider.environment.DB_RST_URL_TABLE}
        AttributeDefinitions:
          - AttributeName: input_rst_name
            AttributeType: S
        KeySchema:
          - AttributeName: input_rst_name
            KeyType: HASH
        ProvisionedThroughput:
          ReadCapacityUnits: 5
          WriteCapacityUnits: 5
    TabelogRstDataTable:
      Type: AWS::DynamoDB::Table
      Properties:
        TableName: ${self:provider.environment.DB_RST_DATA_TABLE}
        AttributeDefinitions:
          - AttributeName: url
            AttributeType: S
        KeySchema:
          - AttributeName: url
            KeyType: HASH
        ProvisionedThroughput:
          ReadCapacityUnits: 5
          WriteCapacityUnits: 5

custom:
  pythonRequirements:
    pythonBin: python3
